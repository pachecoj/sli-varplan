Bayesian machine learning research has paid much attention to
developing posterior inference algorithms for models where posterior
calculation is computationally prohibitive.  Little focus, by
contrast, has been given to methods for decision making based on the
results of inference.  Such problems are of paramount importance in
reinforcement learning (RL), however the context and details of RL
planning differ substantially from our own setting.

In this paper we explore methods for sequential decision making, where
action choices drive data collection.  We further consider the setting
of \emph{information planning}, where decisions are generated by
maximizing the mutual information (MI) utility~\citep{WilliamsThesis}.
The setting most closely resembles Bayesian experiment
design~\citep{lindley56}, where experiments are chosen to minimize
uncertainty over a quantity of interest.  Indeed, MI has long been
used as a design utility since it is a measure of expected reduction
in posterior uncertainty~\citep{blackwell50, bernardo79a}.

Unlike experiment design, which typically assumes the cost of a
measurement dominates that of inference, our focus is on high
throughput sequential decision systems.  Where the former relies on
Monte Carlo inference and MI estimates during planning, we present a
comprehensive approach to inference and planning based on efficient
variational approximations.  

Our approach, which we call variational information planning (VIP),
maintains a series of variational approximations to the posterior and
MI utility.  For the planning stage, VIP extends a well-known lower
bound of MI~\citep{agakov2004algorithm} to the sequential setting.
During planning, the MI lower bound is optimized over an auxiliary
distribution, which approximates the posterior under a hypothesized
measurement.  We demonstrate that this optimization is convex for a
certain class of auxiliary models.  We establish optimality conditions
for the natural parameters of this family, and show that they are a
relaxation of the well known moment matching conditions.

%% Our setting differs from both Bayesian experimental design and RL in a
%% number of ways.  Traditional experiment design typically assumes the
%% cost of observation outweighs that of inference, and thus relies on
%% Markov chain Monte Carlo (MCMC) methods for inference.  MI is then
%% estimated over samples, resulting in estimator bias with slow
%% convergence~\citep{zheng2018robust, rainforth2018nesting}.  In
%% comparison to RL planning, our utility of interest is thus purely
%% exploratory, thus contrasting with the exploration-exploitation
%% trade-off common in RL planning~\citep{sutton1998reinforcement}.
%% Other differences with RL include the use of a structured statistical
%% model and representation of latent variables.

%% Statistical experiment design typically presumes that the cost of
%% experiments, or the cost of observation, far exceed the cost of
%% computation, and therefore rely on Markov chain Monte Carlo (MCMC)
%% methods for computation.  Such an approach is impractical for
%% high-throughput decision systems.  Moreover, Monte Carlo estimates of
%% MI have been shown to be biased due to the use of nested Monte Carlo
%% estimation, and that the rate of bias decay can be
%% slow~\citep{zheng2018robust, rainforth2018nesting}.

A core challenge we face is that variational approximations of
posterior uncertainty can be arbitrarily
poor~\citep{giordano2015linear, turner2011two}, despite their good
predictive accuracy.  Since MI is a measure of uncertainty, a naive
variational approximation will tend to yield poor decisions.  To
address these issues, our auxiliary distributions belong to a set
that, conditioned on a hypothesized observation, are in the
exponential family.  This set enables nonlinear dependence on the
conditioning variable, and is thus strictly larger than the set of
joint exponential family distributions.

In our experiments we demonstrate that VIP is sufficiently flexible to
apply in a variety of problem instances such as nonlinear target
tracking in a sensor network, experiment design, and active learning.
Moreover, VIP meets or exceeds the accuracy of methods based on exact
inference, MCMC requiring more computation, or specialized variational
approximations. 

%% and maximize a
%% well-known lower bound of MI resulting from Gibbs'
%% inequality~\citep{agakov2004algorithm}.  The bound is maximized over
%% an auxiliary model, which is strictly more expressive than the
%% exponential family posterior approximation, thus allowing for better
%% representations of uncertainty.  Moreover, we show that when this
%% class of models is conditionally in the exponential family, the
%% resulting optimization problem is convex.  We also show connections to
%% MI approximation based on moment matching operations, and that it is
%% equivalent to our approach in some restricted settings.  Finally, we
%% demonstrate effectiveness on a variety of problems including nonlinear
%% target tracking in a sensor network, gene regulatory network
%% inference, and active learning in the labeled LDA model (LLDA).


