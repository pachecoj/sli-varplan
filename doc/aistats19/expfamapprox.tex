The previous section characterized natural parameters maximizing the
MI bound~\eqref{eq:varmi_approx} w.r.t.~the auxiliary distribution.
Planning, however, requires the value of this bound at its optimum.
For some models this evaluation is straightforward, but others require
estimation.  We begin with a discussion of computing the bound for
complex models.  We conclude with a class of models for which
evaluation is simple, and corresponds to the standard moment matching
property.

\subsection{Bound Estimation}

%% For simple natural parameter functions, such as linear $\theta(y) =
%% Ay$, the MI bound can often be optimized and evaluated in closed
%% form~\citep{agakov2004algorithm}.  We make extensive use of this
%% approximation for our experiments in \SEC\ref{sec:experiments} for
%% this purpose.  More complicated functions, however, require estimation
%% of the bound.

To simplify the discussion, we focus on the conditionally independent
model with PDF~\eqref{eq:conditional_indep_joint}.  Recall the local
approximation $\hat{p}(x,y) = q(x)p(y\mid x)$, where we drop explicit
time indexing for brevity.  The relevant term in the
bound~\eqref{eq:varmi_approx} is the conditional cross
entropy,
\[
  \EE_{\hat{p}}[ -\log \omega(x \mid y)
  ] \approx - \frac{1}{N} \sum_{i=1}^N \EE_{\hat{p}_{y \mid
    x^i}}[ \log \omega(x^i \mid y) ]
\]
where samples $\{x^i\}_{i=1}^N \sim q(x)$.  Since $q(x)$ is a
tractable distribution, this step can be done efficiently.

The expectation $\EE_{y\mid x^i}[\cdot]$ is with respect to the
forward model (likelihood), and can often be computed in closed-form.
For some models, however, this term must be approximated, and requires
simulation of the forward model.  This step is also efficient,
assuming a Bayesian network, but leads to a higher variance estimate.
Both estimators are consistent by the LLN.

%% A similar approach can
%% be taken for estimating gradients, if closed-form solutions are not
%% available.

\subsection{Moment Matching Solution}\label{sec:moment_match}

%% The reader may consider an alternative approach to approximating MI,
%% which is as follows.  First, select a joint distribution $\omega(x,y)$
%% in the exponential family,
%% \[
%%   \omega_{\eta}(x,y) = h(x,y)\exp\left\{ \eta^T \phi(x,y) - A(\eta)
%%     \right\}.
%% \]
%% Next, approximate the distribution $\hat{p}(x,y)$ by minimizing
%% $\KL{\hat{p}}{\omega}$ via moment matching, $\EE_{\hat{p}}[ \phi(x,y)
%% ] = \EE_{\omega_{\eta}}[ \phi(x,y) ]$.  Finally, approximate mutual
%% information as $I_{\hat{p}}(X;Y) \approx I_{\omega}(X;Y)$.

%% We show how this approach is equivalent to the optimization
%% of \SEC\ref{sec:optim} in some cases.

Under some conditions the value of the MI
bound~\eqref{eq:varmi_approx} takes a simple form at its optimum.  To
see this, we first establish that standard moment matching of the
auxiliary distribution is optimal for some models.  We then show that
the value of the bound at this moment matching solution is equivalent
to calculating entropy under the auxiliary distribution, which is
tractable.

One class of models for which the bound is easily calculated are those
where the marginal $\hat{p}(y)$ is in the exponential family, for
example if $y$ is a discrete label.  Then, consider the following
joint exponential family,
\[
  \omega_{\eta}(x,y) = h(x,y)\exp\left\{ \eta^T \phi(x,y) - A(\eta)
    \right\}.
\]
Furthermore, consider the parameters $\eta^*$ satisfying the moment
matching property,
\begin{equation}\label{eq:moment_match}
  \EE_{\hat{p}}[ \phi(x,y) ] = \EE_{\omega_{\eta}}[ \phi(x,y) ].
\end{equation}
Moment matching, combined with the assumption that $\hat{p}(y)$ is in
the exponential family, implies that the marginal can be exactly
calculated $\omega_{\eta}(y) = \hat{p}(y)$.  Using this equivalence,
and rewriting~\eqref{eq:moment_match}, we have:
\begin{equation}\label{eq:moment_match_cond}
  \EE_{\hat{p}}[ \phi(x,y) ] = \EE_{\hat{p}_y}[ \EE_{\omega_{x\mid y}}[ \phi(x,y)
      \mid Y=y ] ],
\end{equation}
where $\omega_{\eta^{*}}(x\mid y)
= \omega_{\eta^{*}}(x,y) \div \int \omega_{\eta^{*}}(x,y) \deriv
x$.  \EQN\eqref{eq:moment_match_cond} is the optimality
condition~\eqref{eq:stationary_point} of the MI lower bound.  This
establishes that standard moment matching is optimal for the class of
models where $\hat{p}(y)$ is in the exponential family.

We now establish that the moment matching solution $\eta^{*}$ leads to
a simple form of the bound~\eqref{eq:varmi_approx}.  By direct
calculation, the cross entropy $H_{\hat{p}}(\omega_{\eta^*}(x,y))$
equals,
\begin{equation}
  \EE_{\hat{p}}[ - \log h(x,y) ] - \eta^T \EE_{\omega_{\eta^*}}[ \phi(x,y) ] + A(\eta).
\end{equation}
For distributions with constant base measure $h(x,y)$ we have that,
$H_{\hat{p}}(\omega_{\eta^*}(x,y)) = H(\omega_{\eta^*}(x,y))$.  By
similar logic for the marginal entropy, and by applying the entropy
chain rule, we have that:
\begin{equation}
  H_{\hat{p}}( \omega_{\eta^*}(x\mid y) ) = H(\omega_{\eta^*}(x,y)) - H(\omega_{\eta^*}(y)).
\end{equation}
The l.h.s.~is the relevant conditional entropy term from the MI
bound~\eqref{eq:varmi_approx}.  The r.h.s.~is the entropy of the joint
and marginal distributions $\omega(\cdot)$ at the optimal parameters,
which is closed form.  We have thus shown that the aforementioned MI
approximation is equivalent to optimizing the variational lower bound.


%% \subsection{BACKUP}
%% Consider a pair of joint and marginal distributions in the exponential
%% family,
%% \begin{gather*}
%%   \omega_{\eta}(x,y) = h(x,y)\exp\left\{ \eta^T \phi(x,y) - A(\eta)
%%     \right\}\\ % \equiv \omega_{xy}\\
%%   \omega_{\beta}(y) = h(y)\exp\left\{ \beta^T \phi(y) - A(\beta) \right\}.
%%   %\equiv \omega_y.
%% \end{gather*}
%% When $\omega(y) = \int \omega(x,y) \,\deriv x$ are marginally consistent,
%% we have that the conditional is $\omega(x\mid y) = \omega(x,y) \div
%% \omega(y)$.  As a result, the objective $J(\theta)$ can be
%% re-expressed as,
%% \begin{align}\label{eq:constrained_mibound}
%%   &\min_{\eta, \beta} \;J(\eta,\beta) \equiv \EE_p[ \log \omega_{\beta}(Y) ] - \EE_p[ \log
%%     \omega_{\eta}(X,Y) ] \notag \\
%%   &\;\text{s.t.} \; \EE_{\omega_{\beta}}[ \phi(Y) ] =
%%     \EE_{\omega_{\eta}}[ \phi(Y) ].
%% \end{align}
%% We have assumed here that $\int \omega(x,y) \,\deriv x$ is in the same
%% exponential family as $\omega(y)$.  Then, marginal consistency
%% is equivalent to the moment constraints above.  This assumption does not hold in
%% general but will simplify later discussion.

%% When the marginalization constraints are satisfied we have that
%% $J(\eta,\beta) = J(\theta)$ by construction, where $\theta$ can be
%% expressed in terms of the parameters $\eta$ and $\beta$.  The
%% problem~\eqref{eq:constrained_mibound} is then convex on the
%% constraint set, though not strictly so since many joint and marginal
%% distributions map to the same conditional.  

%% The objective $J(\eta,\beta)$ is not convex off of the constraints.
%% By adding constants $\EE_p[ \log p(X,Y) ]$ and $\EE_p[ - \log p(Y) ]$
%% we have the equivalent expression,
%% \begin{equation*}
%%   J(\eta,\beta) = \text{const.} + \KL{ p_{XY} }{ \omega_{\eta} } - \KL{ p_Y }{
%%     \omega_{\beta} },
%% \end{equation*}
%% which is convex in $\eta$ and concave in $\beta$ by convexity
%% properties of Kullback-Leibler for exponential families.

%% The zero gradient of $J(\eta,\beta)$ yields the moment matching
%% equations, 
%% \begin{align}
%%   \EE_p[ \phi(X,Y) ] &= \EE_{\omega_{\eta}}[ \phi(X,Y)
%%     ] \label{eq:statcond_joint} \\
%%   \EE_p[ \phi(Y) ] &= \EE_{\omega_{\beta}}[ \phi(Y) ]. \label{eq:statcond_marg}
%% \end{align}
%% A solution $\{\eta,\beta\}$ to the above equations is a feasible
%% point, given our assumption that $\int \omega(x,y) \,\deriv x$ and
%% $\omega(y)$ belong to the same exponential family.

%% Consider a model where the marginal $p(y)$ is in the exponential
%% family, for example when $p(x,y)$ is a mixture distribution with
%% discrete label $y$.  In this case, the moment matching condition above
%% means $\omega(y) = p(y)$ and thus,
%% \begin{equation}
%%   \EE_p[ \phi(X,Y) ] = \EE_{p_Y}[ \EE_{\omega_{X\mid Y}}[ \phi(X,y)
%%       \mid Y=y ] ],
%% \end{equation}
%% which is the solution of the unconstrained problem in
%% \SEC\ref{sec:optim}.  As a result, it is also a solution to the
%% constrained problem~\eqref{eq:constrained_mibound}, since the
%% objectives are equal on the constraint set by construction.

%% Given the moment matched solution we have that the cross entropy
%% equals, 
%% \[
%%   \EE_p[ - \log \omega_{\eta} ] = \EE_p[ - \log h(x) ] - \eta^T \EE_p[
%%     \phi(X,Y) ] + A(\eta).
%% \]
%% For distributions with constant base measure $h(x)$ the above equals
%% entropy of $\omega(x,y)$, since $\EE_p[ \phi(X,Y) ] = \EE_\omega[
%%   \phi(X,Y) ]$.  The same holds for the marginal entropy and, thus,
%% the conditional cross entropy, \mbox{$H_p( \omega(X\mid Y) ) = H_\omega(X
%% \mid Y)$}.



