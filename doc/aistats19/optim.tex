Given the bound~\eqref{eq:varmi_approx}, it remains to optimize 
the auxiliary $\omega(x\mid y)$, which can be complicated
for arbitrary distributions.  In this section we consider optimization
of this bound for conditional distributions in the exponential family.
We show that the corresponding optimization is convex in the natural
parameters.  Moreover, stationary conditions yield a variation on the
well known moment matching property.

%% We represent conditional exponential families with natural parameters
%% that are themselves functions of the conditioning variable $y$.  With
%% a slight abuse of terminology, we refer to this as a \emph{link
%% function}.  This rich family of distributions is strictly larger than
%% the set of joint distributions $\omega(x,y)$ in the exponential
%% family, and thus allows tighter achievable bounds.  We conclude by
%% characterizing the optimization of link function parameters.


\subsection{Optimizing the Auxiliary Distribution}

Consider the set of exponential family distributions
$\omega \in \Wcal$ with PDF,
\begin{equation}
  \omega_\theta(x \mid y) = h(x)\exp\left( \theta(y)^T \phi(x) - A(\theta(y)) \right),
\end{equation}
with natural parameters $\theta(y)$ a function of the conditioning
variable, sufficient statistics $\phi(x)$, base measure $h(x)$ and
log-partition function $A(\theta(y))$.
Optimizing the bound in \EQN\eqref{eq:varmi_approx} is equivalent to minimizing the
cross entropy,
\begin{equation}\label{eq:crossent}
  \theta^{*}(y) = \argmin_{\theta} J(\theta) \equiv \EE_{\hat{p}}[ - \log \omega_{\theta}(X \mid Y) ].
\end{equation}
Convexity of $J(\theta)$ can be established by explicit calculation of
the Hessian.  Alternatively, adding \mbox{$-H(\hat{p})$} yields the
following problem which is equivalent to $J(\theta)$ up to constant
terms,
\begin{equation}\label{eq:dual}
  \theta^*(y) = \argmin_\theta \EE_{\hat{p}_Y}\left[ \KL{\hat{p}_{X\mid y}}{\omega_\theta} \mid Y=y \right]
\end{equation}
For brevity have introduced the shorthand \mbox{$\hat{p}_{x\mid y} \equiv
\hat{p}(x\mid y)$}.  For any realization $Y=y$ the KL term is convex in
$\theta(y)$, a well known property of the exponential
families~\citep{wainwright_jordan}.  \EQN\eqref{eq:dual} is then a
convex combination of convex functions, thus convexity holds.

The optimal parameter function $\theta^{*}(y)$ is given by the
stationary point condition,
\begin{equation}\label{eq:stationary_point}
  \EE_{\hat{p}_Y}\left[ \EE_{\omega_{\theta^{*}}}[ \phi(X) \mid Y=y ] \right] = \EE_{\hat{p}}[\phi(X)].
\end{equation}
This is a weaker condition than the standard moment matching property
of exponential families, which typically minimizes KL.
Under~\eqref{eq:stationary_point} moments of $\omega(x\mid y)$ must
match \emph{in expectation} w.r.t.~the marginal distribution $p(y)$,
but may not be equal for any particular realization $Y=y$.

%% \begin{gather}
%%   \omega(X \mid Y = y; \theta) = \exp\left( \theta(y)^T \phi(X) -
%%     A(\theta(y)) \right) \\
%%   A(\theta(y)) = \log \int_{\Xcal \times \Ycal} \!\!\!\!\!\exp\left( \theta(y)^T \phi(x) \right)
%% dx dy
%% \end{gather}

\subsection{Parameter Function Optimization}

Stationary conditions~\eqref{eq:stationary_point} are in terms of a
function $\theta(y)$ which is assumed to be parametric.  Let $\eta$ be
parameters of the function, denoted $\theta_{\eta}(y)$.  Stationary
conditions in terms of parameters $\eta$ are then,
\begin{equation*}
  \EE_{p_Y}\left[ \left( D_\eta \theta \right)^T \EE_{\omega_\eta}[\phi(X)]
    \right]
    = \EE_{p_Y}\left[ \left( D_\eta \theta \right)^T \EE_{p_{X\mid
          Y}}[ \theta(X) ] \right]
\end{equation*}
where $D_\eta \theta$ is the Jacobian matrix of partial derivatives.
If $\theta(y)$ is convex in the parameters $\eta$ then the
optimization \EQN\eqref{eq:dual} remains convex.

%% One approach is to
%% represent $\theta_{\eta}(y)$ as a neural network, with parameters
%% $\eta$.  In this setting, the Jacobian can be efficiently calculated
%% for any $y$ via backpropagation.

